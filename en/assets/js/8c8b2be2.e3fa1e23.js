"use strict";(self.webpackChunkrevyos_docs_new=self.webpackChunkrevyos_docs_new||[]).push([[7227],{8453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>r});var a=i(6540);const s={},t=a.createContext(s);function o(e){const n=a.useContext(t);return a.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),a.createElement(t.Provider,{value:n},e.children)}},8578:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>d,contentTitle:()=>r,default:()=>h,frontMatter:()=>o,metadata:()=>a,toc:()=>c});const a=JSON.parse('{"id":"adaptation/GStreamer-pipeline-example-with-thead-omxil-lib","title":"GStreamer Pipeline Example","description":"Some GStreamer pipeline command-line examples with omxil library on TH1520, RevyOS","source":"@site/i18n/en/docusaurus-plugin-content-docs/current/adaptation/GStreamer-pipeline-example-with-thead-omxil-lib.md","sourceDirName":"adaptation","slug":"/adaptation/GStreamer-pipeline-example-with-thead-omxil-lib","permalink":"/revyos-docs-new/en/docs/adaptation/GStreamer-pipeline-example-with-thead-omxil-lib","draft":false,"unlisted":false,"editUrl":"https://github.com/kagura114/revyos-docs-new/docs/adaptation/GStreamer-pipeline-example-with-thead-omxil-lib.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"title":"GStreamer Pipeline Example","sidebar_position":2},"sidebar":"documentSidebar","previous":{"title":"GStreamer Player Porting","permalink":"/revyos-docs-new/en/docs/adaptation/GStreamer\u64ad\u653e\u5668\u9002\u914d"},"next":{"title":"Docker","permalink":"/revyos-docs-new/en/docs/desktop/revyos-use-docker"}}');var s=i(4848),t=i(8453);const o={title:"GStreamer Pipeline Example",sidebar_position:2},r="GStreamer pipeline examples with thead OpenMax-IL lib",d={},c=[{value:"Get ready",id:"get-ready",level:2},{value:"Pipeline samples",id:"pipeline-samples",level:2},{value:"Test",id:"test",level:3},{value:"videotestsrc",id:"videotestsrc",level:4},{value:"fpsdisplaysink",id:"fpsdisplaysink",level:4},{value:"audiotestsrc",id:"audiotestsrc",level:4},{value:"fakesink",id:"fakesink",level:4},{value:"Decode",id:"decode",level:3},{value:"Video decode",id:"video-decode",level:4},{value:"Audio decode",id:"audio-decode",level:4},{value:"demux and decode",id:"demux-and-decode",level:4},{value:"Encode to file",id:"encode-to-file",level:3},{value:"Video encode",id:"video-encode",level:4},{value:"Audio encode",id:"audio-encode",level:4},{value:"mux and encode",id:"mux-and-encode",level:4},{value:"Media file transcode",id:"media-file-transcode",level:3},{value:"Video transcode",id:"video-transcode",level:4},{value:"Audio transcode",id:"audio-transcode",level:4},{value:"Video + audio remux and transcode",id:"video--audio-remux-and-transcode",level:4},{value:"Media mixing",id:"media-mixing",level:3},{value:"Camera capture",id:"camera-capture",level:3},{value:"Stream transfer",id:"stream-transfer",level:3},{value:"Other tools",id:"other-tools",level:3},{value:"gst-inspect-1.0",id:"gst-inspect-10",level:4},{value:"gst-discover1.0",id:"gst-discover10",level:4},{value:"gst-play-1.0",id:"gst-play-10",level:4},{value:"Other examples",id:"other-examples",level:3},{value:"Get ready",id:"get-ready-1",level:2},{value:"Debug tips",id:"debug-tips",level:3},{value:"Grammer",id:"grammer",level:3},{value:"Reference",id:"reference",level:2}];function l(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,t.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"gstreamer-pipeline-examples-with-thead-openmax-il-lib",children:"GStreamer pipeline examples with thead OpenMax-IL lib"})}),"\n",(0,s.jsxs)(n.p,{children:["Some GStreamer pipeline command-line examples with ",(0,s.jsx)(n.strong,{children:"omxil library"})," on ",(0,s.jsx)(n.strong,{children:"TH1520"}),", ",(0,s.jsx)(n.strong,{children:"RevyOS"})]}),"\n",(0,s.jsx)(n.h2,{id:"get-ready",children:"Get ready"}),"\n",(0,s.jsxs)(n.p,{children:["In this section, the grammer of the gstreamer command-line pipeline and some usefule debug tips are introduced. ",(0,s.jsx)(n.strong,{children:"They have been moved to the end of the article."})]}),"\n",(0,s.jsx)(n.h2,{id:"pipeline-samples",children:"Pipeline samples"}),"\n",(0,s.jsxs)(n.p,{children:["Basically, you neet to install ",(0,s.jsx)(n.code,{children:"gstreamer1.0-plugins-base"}),", ",(0,s.jsx)(n.code,{children:"gstreamer1.0-plugins-good"}),", ",(0,s.jsx)(n.code,{children:"gstreamer1.0-plugins-bad"}),", ",(0,s.jsx)(n.code,{children:"gstreamer1.0-omx-generic"}),", ",(0,s.jsx)(n.code,{children:"gstreamer1.0-omx-bellagio-config"}),",",(0,s.jsx)(n.code,{children:"gstreamer1.0-tools"}),"."]}),"\n",(0,s.jsx)(n.h3,{id:"test",children:"Test"}),"\n",(0,s.jsx)(n.h4,{id:"videotestsrc",children:"videotestsrc"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"# videotestsrc\ngst-launch-1.0 videotestsrc ! autovideosink\n\n# specify the video stream format\ngst-launch-1.0 videotestsrc ! video/x-raw, format=NV12, width=960, height=540, framerate=60/1 ! autovideosink\n"})}),"\n",(0,s.jsx)(n.h4,{id:"fpsdisplaysink",children:"fpsdisplaysink"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"# display framerate\ngst-launch-1.0 videotestsrc ! fpsdisplaysink\n\n# no need to sync on the clock - used to test the performance of the pipeline\ngst-launch-1.0 videotestsrc ! fpsdisplaysink sync=false\n\n# stop display on the screen, but redirect the output to stdout\ngst-launch-1.0 videotestsrc ! fpsdisplaysink text-overlay=false -v 2>&1\n\n# specify which sink to use\ngst-launch-1.0 videotestsrc ! fpsdisplaysink sink=glimagesink\n\n# combine the previous examples\ngst-launch-1.0 videotestsrc ! fpsdisplaysink sink=glimagesink sync=false text-overlay=false -v 2>&1\n"})}),"\n",(0,s.jsx)(n.h4,{id:"audiotestsrc",children:"audiotestsrc"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:'# audiotestsrc\ngst-launch-1.0 audiotestsrc ! autoaudiosink\n\n# change volume (0 to 1)\ngst-launch-1.0 audiotestsrc volume=0.1 ! autoaudiosink\n\n# change waveform\n# could be selected among square, silence, pink-noise, etc.\ngst-launch-1.0 audiotestsrc wave=pink-noise ! autoaudiosink\n\n# set fix frequency, such as "do" (262 Hz)\ngst-launch-1.0 audiotestsrc freq=262 ! autoaudiosink\n'})}),"\n",(0,s.jsx)(n.h4,{id:"fakesink",children:"fakesink"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"# a dummy sink that swallows everything\ngst-launch-1.0 videotestsrc ! fakesink\n"})}),"\n",(0,s.jsx)(n.h3,{id:"decode",children:"Decode"}),"\n",(0,s.jsx)(n.h4,{id:"video-decode",children:"Video decode"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"# let decodebin choose which decoder to use,\n# and autovideosink choose which video-sink to use (not recommended)\ngst-launch-1.0 filesrc location=fire.mp4 ! decodebin ! autovideosink\n\n# h264 software decode without opengl display\ngst-launch-1.0 filesrc location=fire.mp4 ! qtdemux ! h264parse ! avdec_h264 ! xvimagesink\n\n# h264 hardware decode with opengl display\ngst-launch-1.0 filesrc location=fire.mp4 ! qtdemux ! h264parse ! omxh264dec ! glimagesink\n\n# h265 hardware decode\ngst-launch-1.0 filesrc location=fire.mp4 ! qtdemux ! h265parse ! omxh265dec ! glimagesink\n\n# vp9 hardware decode \ngst-launch-1.0 filesrc location=fire.webm ! matroskademux ! omxvp9dec ! glimagesink\n\n# mpeg4 hardware decode \ngst-launch-1.0 filesrc location=fire.mp4 ! qtdemux ! queue ! mpeg4videoparse ! omxmpeg4videodec ! glimagesink\n\n# play mkv/webm file\ngst-launch-1.0 filesrc location=fire.mkv ! matroskademux ! decodebin ! glimagesink\n\n# todo 10-bit h264/h265 decode\n"})}),"\n",(0,s.jsx)(n.h4,{id:"audio-decode",children:"Audio decode"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"# there is no hardware decoder on th1520\n\n# let autoaudiosink choose which audio-sink to use\ngst-launch-1.0 filesrc location=blade.mp3 ! decodebin ! audioconvert ! autoaudiosink\n\n# mp3 decode\ngst-launch-1.0 filesrc location=blade.mp3 ! mpegaudioparse ! avdec_mp3 ! audioconvert ! autoaudiosink\n\n# aac decode\ngst-launch-1.0 filesrc location=blade.aac ! aacparse ! avdec_aac ! audioconvert ! autoaudiosink\ngst-launch-1.0 filesrc location=blade.aac ! aacparse ! faad ! audioconvert ! autoaudiosink\n## faad works well without aacparse\ngst-launch-1.0 filesrc location=blade.aac ! faad ! audioconvert ! autoaudiosink\n\n# opus decode\n## ogg file must be demuxed by oggdemux first\ngst-launch-1.0 filesrc location=blade.ogg ! oggdemux ! opusparse ! opusdec ! audioconvert ! autoaudiosink\ngst-launch-1.0 filesrc location=blade.ogg ! oggdemux ! opusparse ! avdec_opus ! audioconvert ! autoaudiosink\n\n# wav decode\ngst-launch-1.0 filesrc location=test.wav ! wavparse ! audioresample ! audioconvert ! autoaudiosink\n\n# use specific audiosink\ngst-launch-1.0 filesrc location=blade.mp3 ! decodebin ! audioconvert ! pulsesink\n\n# specify the output device by using alsasink with device property\ngst-launch-1.0 filesrc location=blade.mp3 ! decodebin ! audioconvert ! alsasink device=hw:0,2\n"})}),"\n",(0,s.jsx)(n.h4,{id:"demux-and-decode",children:"demux and decode"}),"\n",(0,s.jsx)(n.p,{children:"We play media file in this section."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"# play mp4 file with both audio and video\ngst-launch-1.0 filesrc location=fire.mp4 ! qtdemux name=demux \\\n  demux.video_0 ! queue  ! decodebin ! videoconvert ! autovideosink \\\n  demux.audio_0 ! queue ! decodebin ! audioconvert !  autoaudiosink\n"})}),"\n",(0,s.jsx)(n.h3,{id:"encode-to-file",children:"Encode to file"}),"\n",(0,s.jsx)(n.h4,{id:"video-encode",children:"Video encode"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"# h264 encode test\n# before import video stream to omxh264dec, data should be transformed to frame with rawvideoparse\n# property and pad of rawvideoparse should be set to the same, so we use 'use-sink-caps=true' here\ngst-launch-1.0 videotestsrc ! videoconvert \\\n  ! video/x-raw, format=NV12, width=640, height=480 \\\n  ! rawvideoparse use-sink-caps=true ! omxh264enc ! fakesink\n\n# h264 hardware encode to file\n## todo: encoded h264 file has seek problem\ngst-launch-1.0 videotestsrc ! videoconvert \\\n  ! video/x-raw, format=NV12, width=640, height=480 \\\n  ! rawvideoparse use-sink-caps=true ! omxh264enc \\\n  ! h264parse ! qtmux ! mp4mux ! filesink location=test.mp4\n\n# h264 hardware encode to file with specific bitrate(bit per second)\ngst-launch-1.0 videotestsrc ! videoconvert \\\n  ! video/x-raw, format=NV12, width=640, height=480 \\\n  ! rawvideoparse use-sink-caps=true  \\\n  ! omxh264enc target-bitrate=3000000 \\\n  ! h264parse ! filesink location=test.mp4\n\n\n# h265 hardware encode to file\n## this pipeline produces h265 stream only\n## qtdemux is not needed while decoding\ngst-launch-1.0 videotestsrc ! videoconvert \\\n  ! video/x-raw, format=NV12, width=640, height=480 \\\n  ! rawvideoparse use-sink-caps=true ! omxh265enc \\\n  ! h265parse ! filesink location=test.h265\n\n# h264 hardware encode from camera to file\ngst-launch-1.0 v4l2src device=/dev/video0 ! videoconvert \\\n  ! video/x-raw, format=NV12,width=640,height=480 \\\n  ! rawvideoparse use-sink-caps=true \\\n  ! omxh264enc ! h264parse ! filesink location=test.mp4 -e\n"})}),"\n",(0,s.jsx)(n.h4,{id:"audio-encode",children:"Audio encode"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"# There is no hardware audio encoder on th1520\n\n# encode aac stream with adts container(.aac file)\n## the unit of the bitrate is 'bit/sec'\ngst-launch-1.0 audiotestsrc ! voaacenc bitrate=128000 ! avmux_adts ! filesink location=test.aac\n\n\n# todo: encode aac stream with adif container(.m4a file)\n\n# encode to mp3 file\n## the unit of the bitrate is 'kbit/sec'\ngst-launch-1.0 audiotestsrc ! lamemp3enc quality=2 target=bitrate bitrate=192 cbr=true ! id3v2mux ! filesink location=test.mp3\n\n# encode opus stream to .ogg file\ngst-launch-1.0 audiotestsrc ! opusenc ! oggmux ! filesink location=test.opus\n\n\n# todo: encode pcm stream to .wav file\n"})}),"\n",(0,s.jsx)(n.h4,{id:"mux-and-encode",children:"mux and encode"}),"\n",(0,s.jsxs)(n.p,{children:["This part has been removed to ",(0,s.jsx)(n.code,{children:"Video + audio transcode"})," section."]}),"\n",(0,s.jsx)(n.h3,{id:"media-file-transcode",children:"Media file transcode"}),"\n",(0,s.jsx)(n.h4,{id:"video-transcode",children:"Video transcode"}),"\n",(0,s.jsxs)(n.p,{children:["Since ",(0,s.jsx)(n.code,{children:"omx...dec"})," cannot fulfill qtdemux and mp4mux, and gst-omx is no longer maintained, it is hard to mux mp4 file with these two plugins. To mux stream from omxh264dec, use ",(0,s.jsx)(n.code,{children:"avmux_mp4"})," instead. But there is no h265 or vp9 muxer available for ",(0,s.jsx)(n.code,{children:"omx...dec"})]}),"\n",(0,s.jsxs)(n.p,{children:["The raw video stream should be processed by ",(0,s.jsx)(n.code,{children:"rawvideoparse"})," before sent to encoder. Because  itself cannot scale frame or change framerate, ",(0,s.jsx)(n.code,{children:"rawvideoparse"})," need to get the stream info of its sink pad(src pad of the backward element). Therefore ",(0,s.jsx)(n.code,{children:"use-sink-caps"})," must be set to ",(0,s.jsx)(n.code,{children:"true"}),"."]}),"\n",(0,s.jsxs)(n.p,{children:["To change width and height, set properties ",(0,s.jsx)(n.code,{children:"output-width"})," and ",(0,s.jsx)(n.code,{children:"output-height"})," of ",(0,s.jsx)(n.code,{children:"omx...dec"}),". To modify bitrate and bitrate control method, set properties ",(0,s.jsx)(n.code,{children:"control-rate"})," and ",(0,s.jsx)(n.code,{children:"target-bitrate"})," of ",(0,s.jsx)(n.code,{children:"omx...enc"}),". To change framerate, set properties of ",(0,s.jsx)(n.code,{children:"videorate"}),"."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"# h265 to h264\ngst-launch-1.0 filesrc location=test_h265.mp4 ! qtdemux ! h265parse ! omxh265dec \\\n  ! rawvideoparse use-sink-caps=true \\\n  ! omxh264enc ! h264parse ! avmux_mp4 ! filesink location=t_h264.mp4\n\n# vp9 to h264\ngst-launch-1.0 filesrc location=test_vp9.webm ! matroskademux ! omxvp9dec \\\n  ! rawvideoparse use-sink-caps=true \\\n  ! omxh264enc ! h264parse ! avmux_mp4 ! filesink location=t_h264.mp4\n\n# arbitrary input to h264\ngst-launch-1.0 filesrc location=test_h264.mp4 ! decodebin \\\n  ! rawvideoparse use-sink-caps=true \\\n  ! omxh264enc ! h264parse ! filesink location=t_h264.mp4\n\n# h264 to h264, with more options\n## set the video width and height to 1280\xd7720, framerate to 15fps, bitrate mode to constant and bitrate to 5Mbps\ngst-launch-1.0 filesrc location=test_h264.mp4 ! qtdemux ! h264parse \\\n  ! omxh264dec output-width=1280 output-height=720 \\\n  ! videorate ! video/x-raw, framerate=15/1 \\\n  ! rawvideoparse use-sink-caps=true  \\\n  ! omxh264enc control-rate=constant target-bitrate=5000000 ! h264parse ! filesink location=t_h264.mp4\n\n## there is no vp9 encoder in th1520 omxil lib\n"})}),"\n",(0,s.jsx)(n.h4,{id:"audio-transcode",children:"Audio transcode"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"# aac to mp3\ngst-launch-1.0 filesrc location=test.aac ! aacparse ! avdec_aac ! audioconvert ! lamemp3enc quality=2 target=bitrate bitrate=192 cbr=true ! id3v2mux ! filesink location=t.mp3\n\n# mp3 to aac\ngst-launch-1.0 filesrc location=test.mp3 ! mpegaudioparse ! avdec_mp3 ! audioconvert ! voaacenc bitrate=128000 ! avmux_adts ! filesink location=t.aac\n\n# wav to mp3\ngst-launch-1.0 filesrc location=test.wav ! wavparse ! audioresample ! audioconvert ! lamemp3enc quality=2 target=bitrate bitrate=192 cbr=true ! id3v2mux ! filesink location=t.mp3\n\n# mp3 to wav\ngst-launch-1.0 filesrc location=test.mp3 ! mpegaudioparse ! avdec_mp3 ! audioresample ! audioconvert \\\n  ! audio/x-raw, rate=44100, format=S16LE ! wavenc ! filesink location=t.wav\n"})}),"\n",(0,s.jsx)(n.h4,{id:"video--audio-remux-and-transcode",children:"Video + audio remux and transcode"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"# mux test\ngst-launch-1.0 audiotestsrc ! autoaudiosink videotestsrc ! autovideosink\n\n# mux the test video and audio stream to a mp4 file\n## be aware that '-e' must be set to this pipeline,\n## and there is no '!' before audiotestsrc\ngst-launch-1.0 -e videotestsrc ! videoconvert \\\n  ! video/x-raw, format=NV12, width=960, height=540 \\\n  ! rawvideoparse use-sink-caps=true ! omxh264enc ! h264parse \\\n  ! avmux_mp4 name=mux ! filesink location=t_h264.mp4 \\\n  audiotestsrc ! lamemp3enc ! mux.\n\n# change container from mkv to mp4 with h264 stream\n## this means demux a mkv file then mux video and audio stream to mp4 file\ngst-launch-1.0 filesrc location=test_h264.mkv \\\n  ! matroskademux name=demux mp4mux force-create-timecode-trak=true name=mux ! filesink location=t_h264.mp4 \\\n  demux.video_0 ! queue ! video/x-h264 ! mux. \\\n  demux.audio_0 ! queue ! audio/mpeg ! mux.\n"})}),"\n",(0,s.jsx)(n.h3,{id:"media-mixing",children:"Media mixing"}),"\n",(0,s.jsx)(n.h3,{id:"camera-capture",children:"Camera capture"}),"\n",(0,s.jsxs)(n.p,{children:["You can use command ",(0,s.jsx)(n.code,{children:"v4l2-ctl"}),", which is included in package ",(0,s.jsx)(n.code,{children:"v4l-utils"})," to get information of your camera."]}),"\n",(0,s.jsxs)(n.p,{children:["For more information, read ",(0,s.jsx)(n.a,{href:"https://archive.md/nlyBK",children:"this article"})]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"# h264 hardware encode from camera to file\ngst-launch-1.0 v4l2src device=/dev/video0 ! videoconvert \\\n  ! video/x-raw, format=NV12,width=640,height=480 \\\n  ! rawvideoparse format=nv12 width=640 height=480 \\\n  ! omxh264enc ! h264parse ! filesink location=test.mp4 -e\n\n"})}),"\n",(0,s.jsx)(n.h3,{id:"stream-transfer",children:"Stream transfer"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# capture camera and stream to other machine\n\n## source\ngst-launch-1.0 v4l2src device=/dev/video0 !  videoconvert ! videorate \\\n  ! video/x-raw,  format=NV12,width=640,height=480,framerate=20/1 \\\n  ! rawvideoparse format=nv12 width=640 height=480  framerate=20/1 \\\n  ! omxh264enc ! h264parse config-interval=1 ! video/x-h264,stream-format=byte-stream,alignment=nal \\\n  ! rtph264pay ! udpsink  host=192.168.31.27 port=5600\n\n## destination\ngst-launch-1.0 udpsrc port=5600 caps='application/x-rtp, media=(string)video, clock-rate=(int)90000, encoding-name=(string)H264' \\\n  ! rtph264depay ! h264parse ! avdec_h264 ! autovideosink\n"})}),"\n",(0,s.jsx)(n.h3,{id:"other-tools",children:"Other tools"}),"\n",(0,s.jsx)(n.h4,{id:"gst-inspect-10",children:"gst-inspect-1.0"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.code,{children:"gst-inspect-1.0"})," is a tool to print info about a GStreamer element(factory), which is included in ",(0,s.jsx)(n.code,{children:"gstreamer1.0-tools"}),"."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"# print the GStreamer element list with a 'less' like paing filter.\ngst-inspect-1.0\n\n# print info of the element\ngst-inspect-1.0 <element_name>\n"})}),"\n",(0,s.jsx)(n.h4,{id:"gst-discover10",children:"gst-discover1.0"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.code,{children:"gst-discover-1.0"})," is a tool to show info about the media file, which is inclucded in ",(0,s.jsx)(n.code,{children:"gstreamer1.0-plugins-base-apps"}),"."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"gst-discoverer-1.0 -v test.mp4\n"})}),"\n",(0,s.jsx)(n.h4,{id:"gst-play-10",children:"gst-play-1.0"}),"\n",(0,s.jsxs)(n.p,{children:["If you are tired of manually build pipeline for playback by hand. You can use ",(0,s.jsx)(n.code,{children:"gst-play-1.0"})," as an alternative, which is included in ",(0,s.jsx)(n.code,{children:"gstreamer1.0-plugins-base-apps"}),"."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"# play media file\ngst-play-1.0 test.mp4\n\n# play media file with specific sink\ngst-play-1.0 --videosink=glimagesink --audiosink=alsasink\n"})}),"\n",(0,s.jsxs)(n.p,{children:["Left button and right button can be used to seek. For more info, please read ",(0,s.jsx)(n.a,{href:"https://manpages.ubuntu.com/manpages/xenial/man1/gst-play-1.0.1.html",children:"this article"}),"."]}),"\n",(0,s.jsx)(n.h3,{id:"other-examples",children:"Other examples"}),"\n",(0,s.jsx)(n.h2,{id:"get-ready-1",children:"Get ready"}),"\n",(0,s.jsx)(n.h3,{id:"debug-tips",children:"Debug tips"}),"\n",(0,s.jsxs)(n.p,{children:["To get the cap info or property of an element, you may need to run ",(0,s.jsx)(n.code,{children:"gst-inspect-1.0 <element_name>"}),". If ",(0,s.jsx)(n.code,{children:"gst-inspect-1.0"})," command not found, install ",(0,s.jsx)(n.code,{children:"gstreamer1.0-tools"}),"."]}),"\n",(0,s.jsxs)(n.p,{children:["If you want to debug GStreamer, refer to the articles below.\n",(0,s.jsx)(n.a,{href:"https://gstreamer.freedesktop.org/documentation/tutorials/basic/debugging-tools.html",children:"https://gstreamer.freedesktop.org/documentation/tutorials/basic/debugging-tools.html"}),"\n",(0,s.jsx)(n.a,{href:"https://gstreamer.freedesktop.org/documentation/gstreamer/running.html?gi-language=c",children:"https://gstreamer.freedesktop.org/documentation/gstreamer/running.html?gi-language=c"})]}),"\n",(0,s.jsx)(n.h3,{id:"grammer",children:"Grammer"}),"\n",(0,s.jsx)(n.p,{children:"Here is a simple command-line pipeline."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"gst-launch-1.0 videotestsrc ! autovideosink\n"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsxs)(n.strong,{children:[(0,s.jsx)(n.code,{children:"!"})," represent a link."]})," The established pipeline looks like"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-plain",children:"videotestsrc => autovideosink\n"})}),"\n",(0,s.jsxs)(n.p,{children:["There are several properties  in videotestsrc, which could be lookup by ",(0,s.jsx)(n.code,{children:"gst-inspect-1.0 videotestsrc"}),".Set some properties:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"gst-launch-1.0 videotestsrc pattern=ball flip=true ! autovideosink\n"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsxs)(n.strong,{children:[(0,s.jsx)(n.code,{children:"NAME=VALUE"})," is a property."]})," ",(0,s.jsx)(n.code,{children:"pattern"})," and ",(0,s.jsx)(n.code,{children:"flip"})," are properties of videotestsrc."]}),"\n",(0,s.jsxs)(n.p,{children:["Sometimes we need to control the behavior of ",(0,s.jsx)(n.code,{children:"gst-launch-1.0"}),"."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"GST_DEBUG=3 gst-launch-1.0 videotestsrc pattern=ball flip=true ! autovideosink\ngst-launch-1.0 videotestsrc pattern=ball flip=true ! autovideosink --gst-debug-level=3\n"})}),"\n",(0,s.jsxs)(n.p,{children:["There are two ways to achieve this goal. One is setting ",(0,s.jsx)(n.strong,{children:"environment variable"}),". Another one is passing ",(0,s.jsx)(n.strong,{children:"command-line argument"})," to ",(0,s.jsx)(n.code,{children:"gst-launch-1.0"}),". ",(0,s.jsx)(n.code,{children:"GST_DEBUG=3"})," and ",(0,s.jsx)(n.code,{children:"--gst-debug-level=3"})," have the same meaning."]}),"\n",(0,s.jsxs)(n.p,{children:["In most video decoding cases, there is a video-stream and an audio stream. We need to use ",(0,s.jsx)(n.strong,{children:"demuxer"})," to seperate them. Demuxer usually have several src pad. Here is an example."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"gst-launch-1.0 filesrc location=test-video.mp4 ! qtdemux name=demux demux. ! queue ! h264parse ! omxh264dec ! glimagesink demux. ! queue ! aacparse ! avdec_aac ! audioconvert !  alsasink\n"})}),"\n",(0,s.jsx)(n.p,{children:"It is hard to read. Add some word wrap:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"gst-launch-1.0 filesrc location=test-video.mp4 ! qtdemux name=demux \\\n  demux. ! queue ! h264parse ! omxh264dec ! glimagesink \\\n  demux. ! queue ! aacparse ! avdec_aac ! audioconvert ! pulsesink\n"})}),"\n",(0,s.jsx)(n.p,{children:"Here we use a demuxer to seperate video and audio, the grammer is:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-plain",children:"... demuxer name=DEMUXER_NAME \\\nDEMUXER_NAME.PIPELINE_NAME_1 ! queue ! ...(the remaining part of pipeline 1) \\\nDEMUXER_NAME.PIPELINE_NAME_2 ! queue ! ...(the remaining part of pipeline 2)\n"})}),"\n",(0,s.jsxs)(n.p,{children:["Sometimes we need to set certain properties of the stream at certain nodes in the pipeline, e.g set resolution of the videostream of the ",(0,s.jsx)(n.code,{children:"videotestsrc"}),"."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"gst-launch-1.0 videotestsrc ! video/x-raw, width=800, height=600 ! glimagesink\n"})}),"\n",(0,s.jsxs)(n.p,{children:["We negotiated the pad properties between ",(0,s.jsx)(n.code,{children:"videotestsrc"})," and ",(0,s.jsx)(n.code,{children:"glimagesink"}),". The property names and values must be supported by both element. You can find them in ",(0,s.jsx)(n.code,{children:"Pad Templates"})," section of ",(0,s.jsx)(n.code,{children:"gst-inspect-1.0"}),"."]}),"\n",(0,s.jsx)(n.h2,{id:"reference",children:"Reference"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://stackoverflow.com/questions/25382211/",children:"GStreamer pipeline grammar - StackOverflow"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://gist.github.com/hum4n0id/cda96fb07a34300cdb2c0e314c14df0a",children:"GStreamer pipeline samples - GitHub"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://gstreamer.freedesktop.org/documentation/tools/gst-launch.html",children:"gst-launch-1.0 - GStreamer doc"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://stackoverflow.com/questions/73948308/",children:"How to display fps of streaming video in gsteramer? - StackOverflow"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://stackoverflow.com/questions/37496912",children:"Storing AAC Audio and Retrieving - StackOverflow"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://developer.download.nvidia.com/embedded/L4T/r32_Release_v1.0/Docs/Accelerated_GStreamer_User_Guide.pdf",children:"Accelerated GStreamer User Guide - NVIDIA"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://stackoverflow.com/questions/37496912",children:"Storing AAC Audio and Retrieving - StackOverflow"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://stackoverflow.com/questions/70672729",children:"Play an opus file with gstreamer and pulseaudio - StackOverflow"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://forums.raspberrypi.com/viewtopic.php?t=206714&sid=ec650e96fc653a8ad0e06dd099cb9220",children:"mp4mux not working with omxh264enc"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://gitlab.freedesktop.org/gstreamer/gst-omx/-/issues/13",children:'omxh264enc makes qtmux error out with "Buffer has no PTS." - FreeDesktop - Gitlab'})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://gitlab.freedesktop.org/gstreamer/gstreamer/-/merge_requests/4976/",children:"gst-omx: Retire the whole package - FreeDesktop - Gitlab"})}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(l,{...e})}):l(e)}}}]);